{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment/LLM/smart_citation_llm_rater.py\n",
    "\n",
    "from environment.LLM.rater import LLMRater\n",
    "\n",
    "class SmartCitationLLMRater(LLMRater):\n",
    "    def __init__(self, llm, current_items_features_list=[], previous_items_features_list=[], llm_render=False, llm_query_explanation=False):\n",
    "        super().__init__(llm, current_items_features_list, previous_items_features_list, llm_render, llm_query_explanation)\n",
    "        self.request_scale = \"0-9\"\n",
    "        self.system_prompt = (\n",
    "            '''You are simulating a user rating scientific papers.\n",
    "                When rating, you MUST consider the following:\n",
    "\n",
    "                - Topic Matching: Rate higher if paper topics match user's interests.\n",
    "                - Novelty Preference: The user has a novelty preference score from 0 (does not care about recency) to 1 (cares strongly about recency). \n",
    "                You should reward papers with a Normalized Year closer to 1 based on the user's novelty preference.\n",
    "                - Reputability Bias: The user has a reputability bias score from 0 (does not care about citations) to 1 (cares strongly about highly cited papers).\n",
    "                You should reward papers with a Normalized Citations (Reputability) closer to 1 based on the user's reputability bias.\n",
    "                \n",
    "                You must **combine these factors** logically to decide if the user would rate the paper highly or not.\n",
    "                ONLY output a single rating from 0 (not interested) to 9 (extremely interested).\n",
    "'''\n",
    "        )\n",
    "\n",
    "        # Interested in: Web visibility and informetrics. Prefers novelty: 0.74, reputability bias: 0.47\n",
    "    def _get_few_shot_prompts(self):\n",
    "        # (Optional for smarter few-shot prompts later)\n",
    "        return []\n",
    "    def _get_prompt(self, user, item, num_interacted, interactions, retrieved_items):\n",
    "        user_info = (\n",
    "            f\"User Profile:\\n\"\n",
    "            f\" {user.description}\"\n",
    "            # f\"- Novelty Preference (0-1): {user.novelty_preference:.2f}\\n\"\n",
    "            # f\"- Reputability Bias (0-1): {user.reputability_bias:.2f}\\n\"\n",
    "        )\n",
    "\n",
    "        paper_info = (\n",
    "            f\"Paper Details:\\n\"\n",
    "            f\"- Title: {item.title}\\n\"\n",
    "            f\"- Topics: {', '.join(item.topics)}\\n\"\n",
    "            f\"- Normalized Year (0-1): {item.norm_year:.2f}\\n\"\n",
    "            f\"- Normalized Citations (Reputability) (0-1): {item.norm_cite:.2f}\\n\"\n",
    "        )\n",
    "        # print(f\"User Info: {user_info}\")\n",
    "        # print(f\"Paper Info: {paper_info}\")\n",
    "        question = \"Question:\\nHow much would the user like this paper? (ONLY output a number from 0 to 9)\"\n",
    "\n",
    "        full_prompt = f\"{self.system_prompt}\\n\\n{user_info}\\n\\n{paper_info}\\n\\n{question}\"\n",
    "\n",
    "        return [{\"role\": \"user\", \"content\": full_prompt}]\n",
    "\n",
    "    def adjust_rating_in(self, rating):\n",
    "        return rating\n",
    "\n",
    "    def adjust_rating_out(self, rating):\n",
    "        return rating\n",
    "\n",
    "    def adjust_text_in(self, text):\n",
    "        return text\n",
    "\n",
    "\n",
    "# environment/LLM/small_hf_llm.py\n",
    "\n",
    "from environment.LLM.llm import LLM\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "class SmallHuggingfaceLLM(LLM):\n",
    "    def __init__(self, model_name=\"google/flan-t5-large\", device=\"cpu\"):\n",
    "        super().__init__(model_name)\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def _generate(self, prompt):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return text\n",
    "\n",
    "    def request_rating_0_9(self, system_prompt, dialog):\n",
    "        prompt = self._dialog_to_text(dialog)\n",
    "        response = self._generate(prompt)\n",
    "        return prompt, self._extract_number(response, 0, 9)\n",
    "\n",
    "    def request_rating_1_10(self, system_prompt, dialog):\n",
    "        prompt = self._dialog_to_text(dialog)\n",
    "        response = self._generate(prompt)\n",
    "        return prompt, self._extract_number(response, 1, 10)\n",
    "\n",
    "    def request_rating_text(self, system_prompt, dialog):\n",
    "        prompt = self._dialog_to_text(dialog)\n",
    "        response = self._generate(prompt)\n",
    "        return prompt, response\n",
    "\n",
    "    def request_explanation(self, system_prompt, dialog):\n",
    "        prompt = self._dialog_to_text(dialog)\n",
    "        response = self._generate(prompt)\n",
    "        return prompt, response\n",
    "\n",
    "    def _dialog_to_text(self, dialog):\n",
    "        return \"\\n\".join([f\"{d['role'].capitalize()}: {d['content']}\" for d in dialog])\n",
    "\n",
    "    def _extract_number(self, text, min_val, max_val):\n",
    "        import re\n",
    "        numbers = re.findall(r'\\d+', text)\n",
    "        for num in numbers:\n",
    "            n = int(num)\n",
    "            if min_val <= n <= max_val:\n",
    "                return str(n)\n",
    "        return str((min_val + max_val) // 2)  # fallback: middle value\n",
    "\n",
    "\n",
    "\n",
    "llm = SmallHuggingfaceLLM(model_name=\"google/flan-t5-small\", device=\"cpu\")\n",
    "rater = SmartCitationLLMRater(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))  \n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from environment.env import Simulatio4RecSys\n",
    "from environment.citations.citation_loader import CitationsLoader\n",
    "from environment.users.citation_users_loader import CitationUsersLoader\n",
    "from environment.citations.citations_retrieval import CitationsRetrieval\n",
    "from environment.items_selection import GreedySelector\n",
    "from environment.reward_shaping import IdentityRewardShaping\n",
    "from environment.reward_perturbator import NoPerturbator\n",
    "from environment.flat_obs_wrapper import FlatObsWrapper\n",
    "from environment.LLM.rater import DummyLLMRater\n",
    "from environment.LLM.dummy_llm import DummyLLM\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "\n",
    "csv_path = \"../environment/citations/datasets/cleaned-scientometrics-and-bibliometrics-research.csv\"\n",
    "users_path = \"../environment/users/datasets/citation_users.json\"\n",
    "\n",
    "# Load\n",
    "items_loader = CitationsLoader(csv_path)\n",
    "users_loader = CitationUsersLoader(users_path)\n",
    "\n",
    "raw_env = Simulatio4RecSys(\n",
    "    render_mode=\"human\",\n",
    "    items_loader=items_loader,\n",
    "    users_loader=users_loader,\n",
    "    items_selector=GreedySelector(),\n",
    "    reward_perturbator=NoPerturbator(),\n",
    "    items_retrieval=CitationsRetrieval(),\n",
    "    reward_shaping=IdentityRewardShaping(),\n",
    "    llm_rater=rater\n",
    ")\n",
    "\n",
    "env = FlatObsWrapper(raw_env)\n",
    "env = Monitor(env)\n",
    "\n",
    "# Load model\n",
    "model = DQN.load(\"models/dqn_citation_recommender_llm\", env=env, device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_precision_at_k(env, model, rater, users_loader, K=5, threshold=6):\n",
    "    \"\"\"\n",
    "    Evaluate Precision@K across all users.\n",
    "\n",
    "    Args:\n",
    "        env: The environment (wrapped with Monitor, FlatObsWrapper, etc.)\n",
    "        model: Trained RL model (e.g., DQN, A2C)\n",
    "        rater: LLMRater or RuleBasedRater to simulate user feedback\n",
    "        users_loader: Loader to access users\n",
    "        K: Number of top recommendations\n",
    "        threshold: Minimum score to count as a \"click\"\n",
    "\n",
    "    Returns:\n",
    "        mean_precision: Mean precision@K across users\n",
    "    \"\"\"\n",
    "\n",
    "    all_precisions = []\n",
    "    n_users = len(users_loader.get_users())\n",
    "\n",
    "    for user_idx in range(n_users)[:2]:\n",
    "        # Reset user\n",
    "        obs = env.reset()[0]\n",
    "        user = users_loader.get_users()[user_idx]\n",
    "\n",
    "        clicked = []\n",
    "\n",
    "        for _ in range(K):\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            action = int(action)\n",
    "            item_id = env.unwrapped.action_to_item[action]\n",
    "            paper = env.unwrapped.items_loader.load_items_from_ids([item_id])[0]\n",
    "            print(f\"User {user_idx}: Paper ID: {item_id}, Title: {paper.title}\")\n",
    "\n",
    "            # Get simulated \"rating\" from user for this paper\n",
    "            rating, _, _ = rater.query(user, paper, 0, [], [])\n",
    "\n",
    "            # Check if user \"clicked\" (liked it)\n",
    "            click = (rating >= threshold)\n",
    "            clicked.append(click)\n",
    "\n",
    "            # Step environment\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        precision = np.sum(clicked) / K\n",
    "        all_precisions.append(precision)\n",
    "\n",
    "    mean_precision = np.mean(all_precisions)\n",
    "    return mean_precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 0: Paper ID: 825, Title: Is commercialization good or bad for science? Individual-level evidence from the Max Planck Society\n",
      "User 0: Paper ID: 825, Title: Is commercialization good or bad for science? Individual-level evidence from the Max Planck Society\n",
      "User 0: Paper ID: 825, Title: Is commercialization good or bad for science? Individual-level evidence from the Max Planck Society\n",
      "User 0: Paper ID: 825, Title: Is commercialization good or bad for science? Individual-level evidence from the Max Planck Society\n",
      "User 0: Paper ID: 825, Title: Is commercialization good or bad for science? Individual-level evidence from the Max Planck Society\n",
      "User 1: Paper ID: 825, Title: Is commercialization good or bad for science? Individual-level evidence from the Max Planck Society\n",
      "User 1: Paper ID: 825, Title: Is commercialization good or bad for science? Individual-level evidence from the Max Planck Society\n",
      "User 1: Paper ID: 825, Title: Is commercialization good or bad for science? Individual-level evidence from the Max Planck Society\n",
      "User 1: Paper ID: 825, Title: Is commercialization good or bad for science? Individual-level evidence from the Max Planck Society\n",
      "User 1: Paper ID: 825, Title: Is commercialization good or bad for science? Individual-level evidence from the Max Planck Society\n",
      "📊 Precision@5 = 1.0000\n"
     ]
    }
   ],
   "source": [
    "mean_precision = evaluate_precision_at_k(env, model, rater, users_loader, K=5, threshold=1)\n",
    "print(f\"📊 Precision@5 = {mean_precision:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔵 User 0: Interested in: Web visibility and informetrics. Prefers novelty: 0.74, reputability bias: 0.47\n",
      "    ➔ Recommendation 1: Paper ID 825 - Is commercialization good or bad for science? Individual-level evidence from the Max Planck Society\n",
      "    ➔ Recommendation 2: Paper ID 14589 - PROCEEDINGS OF THE AMERICAN PHYSIOLOGICAL SOCIETY\n",
      "    ➔ Recommendation 3: Paper ID 8507 - Metrics of activity in social networks are correlated with traditional metrics of scientific impact in endocrinology journals\n",
      "    ➔ Recommendation 4: Paper ID 40492 - Casey, Authority on Ink Chemistry, Wins Iowa Award\n",
      "    ➔ Recommendation 5: Paper ID 43716 - You set the standards\n",
      "\n",
      "🔵 User 1: Interested in: Science and Science Education, Intellectual Capital and Performance Analysis, Innovation Policy and R&D. Prefers novelty: 0.5, reputability bias: 0.65\n",
      "    ➔ Recommendation 1: Paper ID 825 - Is commercialization good or bad for science? Individual-level evidence from the Max Planck Society\n",
      "    ➔ Recommendation 2: Paper ID 35234 - Issue Publication Information\n",
      "    ➔ Recommendation 3: Paper ID 35894 - Decent writing on academic publishing using bibliometrics\n",
      "    ➔ Recommendation 4: Paper ID 5357 - Refrain from adopting the combination of citation and journal metrics to grade publications, as used in the Italian national research assessment exercise (VQR 2011–2014)\n",
      "    ➔ Recommendation 5: Paper ID 26675 - Editorial for January 2011 for <i>JPC A/B/C</i>\n",
      "\n",
      "📊 Precision@5: 0.3000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_precision_at_k(env, model, rater, users_loader, K=5, threshold=6):\n",
    "    \"\"\"\n",
    "    Evaluate Precision@K across first `num_users_eval` users.\n",
    "\n",
    "    Args:\n",
    "        env: The environment (wrapped with Monitor, FlatObsWrapper, etc.)\n",
    "        model: Trained RL model (e.g., DQN, A2C)\n",
    "        rater: LLMRater or RuleBasedRater to simulate user feedback\n",
    "        users_loader: Loader to access users\n",
    "        K: Number of top recommendations\n",
    "        threshold: Minimum score to count as a \"click\"\n",
    "        num_users_eval: Number of users to evaluate (default 2)\n",
    "\n",
    "    Returns:\n",
    "        mean_precision: Mean precision@K across users\n",
    "    \"\"\"\n",
    "\n",
    "    all_precisions = []\n",
    "    n_users = len(users_loader.get_users())\n",
    "    # num_users_eval = min(num_users_eval, n_users)\n",
    "\n",
    "    for user_idx in range(n_users)[:2]:\n",
    "        obs = env.reset()[0]\n",
    "        user = users_loader.get_users()[user_idx]\n",
    "\n",
    "        clicked = []\n",
    "        recommended_actions = set()\n",
    "\n",
    "        print(f\"\\n🔵 User {user_idx}: {user.description}\")\n",
    "\n",
    "        for rec_idx in range(K):\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            action = int(action)\n",
    "\n",
    "            # Avoid recommending the same paper again\n",
    "            while action in recommended_actions:\n",
    "                action = env.action_space.sample()\n",
    "                action = int(action)\n",
    "\n",
    "            recommended_actions.add(action)\n",
    "\n",
    "            item_id = env.unwrapped.action_to_item[action]\n",
    "            paper = env.unwrapped.items_loader.load_items_from_ids([item_id])[0]\n",
    "\n",
    "            print(f\"    ➔ Recommendation {rec_idx+1}: Paper ID {item_id} - {paper.title}\")\n",
    "\n",
    "            # Get simulated \"rating\" from user for this paper\n",
    "            rating, _, _ = rater.query(user, paper, 0, [], [])\n",
    "\n",
    "            # Check if user \"clicked\" (liked it)\n",
    "            click = (rating >= threshold)\n",
    "            clicked.append(click)\n",
    "\n",
    "            # Step environment\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            if done or truncated:\n",
    "                obs = env.reset()[0]\n",
    "\n",
    "        precision = np.sum(clicked) / K\n",
    "        all_precisions.append(precision)\n",
    "\n",
    "    mean_precision = np.mean(all_precisions)\n",
    "    print(f\"\\n📊 Precision@{K}: {mean_precision:.4f}\")\n",
    "    return mean_precision\n",
    "mean_precision = evaluate_precision_at_k(env, model, rater, users_loader, K=5, threshold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
